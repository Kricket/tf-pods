{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'QPodEnvironment' object has no attribute '_get_pod'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-409f6b07f368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQPodEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPodBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_py_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"valid!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pip/tf_agents/environments/utils.py\u001b[0m in \u001b[0;36mvalidate_py_environment\u001b[0;34m(environment, episodes)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0mepisode_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m   \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mepisode_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/pip/tf_agents/environments/py_environment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m           \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \"\"\"\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/qnetwork/qpod_env.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_ended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/qnetwork/qpod_env.py\u001b[0m in \u001b[0;36m_to_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_to_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mpod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# All values here are in the game frame of reference. We do the rotation at the end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QPodEnvironment' object has no attribute '_get_pod'"
     ]
    }
   ],
   "source": [
    "from tf_agents.environments import utils\n",
    "from qnetwork.qpod_env import QPodEnvironment\n",
    "from pod.board import PodBoard\n",
    "\n",
    "environment = QPodEnvironment(PodBoard())\n",
    "utils.validate_py_environment(environment, episodes=5)\n",
    "\n",
    "print(\"valid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Beware: Copying a layer that has already been built: 'input_1'.  This can lead to subtle bugs because the original layer's weights will not be used in the copy.\n",
      "WARNING:absl:Beware: Copying a layer that has already been built: 'input_1'.  This can lead to subtle bugs because the original layer's weights will not be used in the copy.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.environments import wrappers\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tensorflow.python.keras.engine.input_layer import InputLayer\n",
    "\n",
    "\n",
    "\n",
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}\n",
    "\n",
    "\n",
    "\n",
    "environment.reset()\n",
    "\n",
    "env = wrappers.ActionDiscretizeWrapper(environment, num_actions=100)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(QPodEnvironment(PodWorld()))\n",
    "eval_env = tf_py_environment.TFPyEnvironment(QPodEnvironment(PodWorld()))\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "preprocessing_combiner = InputLayer(\n",
    "    input_shape=(6,)\n",
    ")\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params,\n",
    "    preprocessing_combiner=preprocessing_combiner)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())\n",
    "time_step = train_env.reset()\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 2.6881228612262953e+18\n",
      "step = 400: loss = 3.9356166594730394e+18\n",
      "step = 600: loss = 4.78967825926665e+18\n",
      "step = 800: loss = 6.893680070678807e+18\n",
      "step = 1000: loss = 7.454886198658466e+18\n",
      "step = 1000: Average Return = -96385744896.0\n",
      "step = 1200: loss = 7.680946888840839e+18\n",
      "step = 1400: loss = 9.044319866806338e+18\n",
      "step = 1600: loss = 1.0457745362847269e+19\n",
      "step = 1800: loss = 1.4396994258982666e+19\n",
      "step = 2000: loss = 1.76328635765972e+19\n",
      "step = 2000: Average Return = -96385744896.0\n",
      "step = 2200: loss = 2.4202771008000623e+19\n",
      "step = 2400: loss = 3.4125650143423234e+19\n",
      "step = 2600: loss = 4.336984473148483e+19\n",
      "step = 2800: loss = 5.270775509362829e+19\n",
      "step = 3000: loss = 7.46462226417126e+19\n",
      "step = 3000: Average Return = -96385744896.0\n",
      "step = 3200: loss = 7.979405691812263e+19\n",
      "step = 3400: loss = 9.100931301594943e+19\n",
      "step = 3600: loss = 1.1979684959968297e+20\n",
      "step = 3800: loss = 1.0467878462008852e+20\n",
      "step = 4000: loss = 1.263436672869031e+20\n",
      "step = 4000: Average Return = -96385744896.0\n",
      "step = 4200: loss = 1.4367191776409472e+20\n",
      "step = 4400: loss = 1.7030383101524784e+20\n",
      "step = 4600: loss = 2.248124630098709e+20\n",
      "step = 4800: loss = 2.1747087433762918e+20\n",
      "step = 5000: loss = 2.847728782752864e+20\n",
      "step = 5000: Average Return = -96385744896.0\n",
      "step = 5200: loss = 3.4447245419822134e+20\n",
      "step = 5400: loss = 4.024047894798784e+20\n",
      "step = 5600: loss = 3.6152621379656194e+20\n",
      "step = 5800: loss = 4.4687980612499905e+20\n",
      "step = 6000: loss = 4.661337500631706e+20\n",
      "step = 6000: Average Return = -96385744896.0\n",
      "step = 6200: loss = 6.844912002086941e+20\n",
      "step = 6400: loss = 6.103244438015291e+20\n",
      "step = 6600: loss = 6.844795189971607e+20\n",
      "step = 6800: loss = 8.29490430817654e+20\n",
      "step = 7000: loss = 8.436192080111337e+20\n",
      "step = 7000: Average Return = -96385744896.0\n",
      "step = 7200: loss = 9.375243267913894e+20\n",
      "step = 7400: loss = 8.394694224294885e+20\n",
      "step = 7600: loss = 1.3149508861004758e+21\n",
      "step = 7800: loss = 1.1497110010224852e+21\n",
      "step = 8000: loss = 1.366525827558146e+21\n",
      "step = 8000: Average Return = -96385744896.0\n",
      "step = 8200: loss = 1.3668529014810838e+21\n",
      "step = 8400: loss = 1.62825786792734e+21\n",
      "step = 8600: loss = 1.4938135971013088e+21\n",
      "step = 8800: loss = 1.9264189627323803e+21\n",
      "step = 9000: loss = 2.1437080746037986e+21\n",
      "step = 9000: Average Return = -96385744896.0\n",
      "step = 9200: loss = 2.0179126853871558e+21\n",
      "step = 9400: loss = 2.2656658339879683e+21\n",
      "step = 9600: loss = 2.1218540759370066e+21\n",
      "step = 9800: loss = 2.8796432600372483e+21\n",
      "step = 10000: loss = 2.8044266723356967e+21\n",
      "step = 10000: Average Return = -96385744896.0\n",
      "step = 10200: loss = 2.1250344616988603e+21\n",
      "step = 10400: loss = 2.6351912499632983e+21\n",
      "step = 10600: loss = 3.6397599607199105e+21\n",
      "step = 10800: loss = 3.183536592743001e+21\n",
      "step = 11000: loss = 3.5634231026110505e+21\n",
      "step = 11000: Average Return = -96385744896.0\n",
      "step = 11200: loss = 3.811175125311956e+21\n",
      "step = 11400: loss = 4.003853754069655e+21\n",
      "step = 11600: loss = 5.241945645979472e+21\n",
      "step = 11800: loss = 5.217608193593162e+21\n",
      "step = 12000: loss = 4.91414776940159e+21\n",
      "step = 12000: Average Return = -96385744896.0\n",
      "step = 12200: loss = 6.000824701189538e+21\n",
      "step = 12400: loss = 4.97209783760678e+21\n",
      "step = 12600: loss = 5.345556021956712e+21\n",
      "step = 12800: loss = 5.836939273699479e+21\n",
      "step = 13000: loss = 6.080161801075157e+21\n",
      "step = 13000: Average Return = -96385744896.0\n",
      "step = 13200: loss = 6.598251961558185e+21\n",
      "step = 13400: loss = 7.551228279408571e+21\n",
      "step = 13600: loss = 7.557334597553332e+21\n",
      "step = 13800: loss = 7.998529172098729e+21\n",
      "step = 14000: loss = 8.685901198325312e+21\n",
      "step = 14000: Average Return = -96385744896.0\n",
      "step = 14200: loss = 9.513334047063212e+21\n",
      "step = 14400: loss = 6.148766823048752e+21\n",
      "step = 14600: loss = 7.574808001157576e+21\n",
      "step = 14800: loss = 9.813171325354564e+21\n",
      "step = 15000: loss = 9.972435496676987e+21\n",
      "step = 15000: Average Return = -96385744896.0\n",
      "step = 15200: loss = 1.0179330863558388e+22\n",
      "step = 15400: loss = 1.1261087612753433e+22\n",
      "step = 15600: loss = 1.0834735213729989e+22\n",
      "step = 15800: loss = 1.1003087649100446e+22\n",
      "step = 16000: loss = 1.25786201873403e+22\n",
      "step = 16000: Average Return = -96385744896.0\n",
      "step = 16200: loss = 1.4169108054042743e+22\n",
      "step = 16400: loss = 1.4990396866989002e+22\n",
      "step = 16600: loss = 1.4031020934068122e+22\n",
      "step = 16800: loss = 1.4606999801211417e+22\n",
      "step = 17000: loss = 1.3352520999107459e+22\n",
      "step = 17000: Average Return = -96385744896.0\n",
      "step = 17200: loss = 1.4504944855955481e+22\n",
      "step = 17400: loss = 1.7729342045167662e+22\n",
      "step = 17600: loss = 1.7266654604350592e+22\n",
      "step = 17800: loss = 1.6409250300092542e+22\n",
      "step = 18000: loss = 1.7470218433407645e+22\n",
      "step = 18000: Average Return = -96385744896.0\n",
      "step = 18200: loss = 1.7427698823425734e+22\n",
      "step = 18400: loss = 1.7153925002077881e+22\n",
      "step = 18600: loss = 2.0555878858398957e+22\n",
      "step = 18800: loss = 2.0901861144372254e+22\n",
      "step = 19000: loss = 2.1229185017089356e+22\n",
      "step = 19000: Average Return = -96385744896.0\n",
      "step = 19200: loss = 2.1856047804622496e+22\n",
      "step = 19400: loss = 2.515329522140702e+22\n",
      "step = 19600: loss = 2.4356349490346607e+22\n",
      "step = 19800: loss = 2.347872852736129e+22\n",
      "step = 20000: loss = 2.933153454549795e+22\n",
      "step = 20000: Average Return = -96385744896.0\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-101686960865.28, 250.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
