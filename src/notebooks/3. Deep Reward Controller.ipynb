{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning of rewards\n",
    "\n",
    "This illustrates the **DeepRewardController**. This controller uses a neural network to attempt to predict the action with the highest reward value. In other words, a perfectly-trained controller would be identical to the **RewardController**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pod.board import PodBoard\n",
    "from pod.ai.deep_reward_controller import DeepRewardController\n",
    "\n",
    "board = PodBoard()\n",
    "controller = DeepRewardController(board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "First, we create some training data: a bunch of pods in various states around the target checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 pods generated\n",
      "72000 pods generated\n",
      "132000 total states\n"
     ]
    }
   ],
   "source": [
    "from pod.ai.ai_utils import gen_pods, MAX_VEL\n",
    "from pod.constants import Constants\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "pods_everywhere = gen_pods(\n",
    "    board.checkpoints[0],\n",
    "    np.arange(Constants.check_radius(), 10000, 1000),\n",
    "    np.arange(math.pi * -0.9, math.pi * 0.91, math.pi * 0.2),\n",
    "    np.arange(math.pi * -0.9, math.pi * 0.91, math.pi * 0.2),\n",
    "    np.arange(0, MAX_VEL + 1, MAX_VEL / 5)\n",
    ")\n",
    "\n",
    "# TODO: training goes much better if I add extra pods pointing towards the check...why?\n",
    "pods_focused = gen_pods(\n",
    "    board.checkpoints[0],\n",
    "    np.arange(Constants.check_radius(), 10000, 1000),\n",
    "    np.arange(-0.3, 0.3, 0.05),\n",
    "    np.arange(math.pi * -0.9, math.pi * 0.91, math.pi * 0.2),\n",
    "    np.arange(0, MAX_VEL + 1, MAX_VEL / 5)\n",
    ")\n",
    "\n",
    "pods = [*pods_everywhere, *pods_focused]\n",
    "\n",
    "print(\"{} total states\".format(len(pods)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a bunch of pod states, we can perform the training. The labels (i.e. the target output for each state) are calculated as whatever produces the highest reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4125/4125 [==============================] - 3s 570us/step - loss: 1.4657 - accuracy: 0.3035\n",
      "Epoch 2/50\n",
      "4125/4125 [==============================] - 2s 577us/step - loss: 0.9748 - accuracy: 0.5835\n",
      "Epoch 3/50\n",
      "4125/4125 [==============================] - 2s 567us/step - loss: 0.4714 - accuracy: 0.92040s - l\n",
      "Epoch 4/50\n",
      "4125/4125 [==============================] - 2s 558us/step - loss: 0.3097 - accuracy: 0.9420\n",
      "Epoch 5/50\n",
      "4125/4125 [==============================] - 2s 571us/step - loss: 0.2367 - accuracy: 0.9465\n",
      "Epoch 6/50\n",
      "4125/4125 [==============================] - 2s 602us/step - loss: 0.1972 - accuracy: 0.9547\n",
      "Epoch 7/50\n",
      "4125/4125 [==============================] - 2s 583us/step - loss: 0.1737 - accuracy: 0.9628\n",
      "Epoch 8/50\n",
      "4125/4125 [==============================] - 2s 561us/step - loss: 0.1566 - accuracy: 0.9672\n",
      "Epoch 9/50\n",
      "4125/4125 [==============================] - 2s 551us/step - loss: 0.1434 - accuracy: 0.9711\n",
      "Epoch 10/50\n",
      "4125/4125 [==============================] - 2s 542us/step - loss: 0.1322 - accuracy: 0.9747\n",
      "Epoch 11/50\n",
      "4125/4125 [==============================] - 2s 579us/step - loss: 0.1254 - accuracy: 0.9761\n",
      "Epoch 12/50\n",
      "4125/4125 [==============================] - 2s 570us/step - loss: 0.1183 - accuracy: 0.9785\n",
      "Epoch 13/50\n",
      "4125/4125 [==============================] - 2s 566us/step - loss: 0.1126 - accuracy: 0.9805\n",
      "Epoch 14/50\n",
      "4125/4125 [==============================] - 2s 602us/step - loss: 0.1095 - accuracy: 0.9824\n",
      "Epoch 15/50\n",
      "4125/4125 [==============================] - 2s 593us/step - loss: 0.1039 - accuracy: 0.9829\n",
      "Epoch 16/50\n",
      "4125/4125 [==============================] - 2s 540us/step - loss: 0.1010 - accuracy: 0.9841\n",
      "Epoch 17/50\n",
      "4125/4125 [==============================] - 2s 547us/step - loss: 0.0957 - accuracy: 0.9851\n",
      "Epoch 18/50\n",
      "4125/4125 [==============================] - 2s 562us/step - loss: 0.0961 - accuracy: 0.9841\n",
      "Epoch 19/50\n",
      "4125/4125 [==============================] - 2s 576us/step - loss: 0.0913 - accuracy: 0.9860\n",
      "Epoch 20/50\n",
      "4125/4125 [==============================] - 2s 580us/step - loss: 0.0894 - accuracy: 0.9865\n",
      "Epoch 21/50\n",
      "4125/4125 [==============================] - 2s 579us/step - loss: 0.0864 - accuracy: 0.9874\n",
      "Epoch 22/50\n",
      "4125/4125 [==============================] - 2s 545us/step - loss: 0.0859 - accuracy: 0.9874\n",
      "Epoch 23/50\n",
      "4125/4125 [==============================] - 2s 548us/step - loss: 0.0821 - accuracy: 0.9880\n",
      "Epoch 24/50\n",
      "4125/4125 [==============================] - 2s 565us/step - loss: 0.0817 - accuracy: 0.9885\n",
      "Epoch 25/50\n",
      "4125/4125 [==============================] - 2s 587us/step - loss: 0.0802 - accuracy: 0.9889\n",
      "Epoch 26/50\n",
      "4125/4125 [==============================] - 2s 571us/step - loss: 0.0791 - accuracy: 0.9887\n",
      "Epoch 27/50\n",
      "4125/4125 [==============================] - 2s 588us/step - loss: 0.0776 - accuracy: 0.9887\n",
      "Epoch 28/50\n",
      "4125/4125 [==============================] - 2s 557us/step - loss: 0.0761 - accuracy: 0.9894\n",
      "Epoch 29/50\n",
      "4125/4125 [==============================] - 2s 569us/step - loss: 0.0756 - accuracy: 0.98890s\n",
      "Epoch 30/50\n",
      "2086/4125 [==============>...............] - ETA: 1s - loss: 0.0743 - accuracy: 0.9894"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = controller.train(pods, 50)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "#plt.plot(history.history['loss'])\n",
    "plt.legend([\n",
    "    \"Accuracy\",\n",
    "#    \"Loss\"\n",
    "])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play\n",
    "\n",
    "Now that the model has been trained, let's see what it can do!\n",
    "\n",
    "As a comparison, we also add a **SimpleController** (which simply goes full-speed toward the next checkpoint) and **RewardController** (which takes whatever action produces the highest reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TURNS = 200\n",
    "\n",
    "from pod.util import PodState\n",
    "from pod.game import Player\n",
    "from pod.drawer import Drawer\n",
    "from IPython.display import Image\n",
    "from pod.ai.reward_controller import RewardController\n",
    "from pod.controller import SimpleController\n",
    "\n",
    "deep_player = Player(controller)\n",
    "base_player = Player(RewardController(board))\n",
    "simple_player = Player(SimpleController())\n",
    "\n",
    "drawer = Drawer(board, [deep_player, base_player, simple_player])\n",
    "\n",
    "deep_player.reset(board)\n",
    "base_player.reset(board)\n",
    "simple_player.reset(board)\n",
    "\n",
    "file = '/tf/notebooks/pods.gif'\n",
    "drawer.animate(file, TURNS)\n",
    "Image(filename = file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the rewards for the two players in the above run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_player.reset(board)\n",
    "base_player.reset(board)\n",
    "simple_player.reset(board)\n",
    "\n",
    "drawer.chart_rewards(TURNS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
