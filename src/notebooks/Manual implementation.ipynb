{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "from tensorflow.keras.callbacks import History\n",
    "\n",
    "from vec2 import ORIGIN, EPSILON\n",
    "from constants import Constants\n",
    "from podworld import PodWorld\n",
    "from podutil import PodInfo, Controller, PlayInput, PlayOutput, clean_angle\n",
    "\n",
    "print(\"Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup\n"
     ]
    }
   ],
   "source": [
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "\n",
    "GAMMA = 0.95\n",
    "LAMBDA = 0.0005\n",
    "TAU = 0.08\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "REWARD_STD = 1.0\n",
    "\n",
    "print(\"Setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, maxlen = 2000):\n",
    "        self._buffer = deque(maxlen=maxlen)\n",
    "    \n",
    "    def store(self, state, action, reward, next_state, terminated):\n",
    "        self._buffer.append((state, action, reward, next_state, terminated))\n",
    "              \n",
    "    def get_batch(self, batch_size):\n",
    "        if no_samples > len(self._samples):\n",
    "            return random.sample(self._buffer, len(self._samples))\n",
    "        else:\n",
    "            return random.sample(self._buffer, batch_size)\n",
    "        \n",
    "    def get_arrays_from_batch(self, batch):\n",
    "        states = np.array([x[0] for x in batch])\n",
    "        actions = np.array([x[1] for x in batch])\n",
    "        rewards = np.array([x[2] for x in batch])\n",
    "        next_states = np.array([(np.zeros(NUM_STATES) if x[3] is None else x[3]) \n",
    "                                for x in batch])\n",
    "        \n",
    "        return states, actions, rewards, next_states\n",
    "        \n",
    "    @property\n",
    "    def buffer_size(self):\n",
    "        return len(self._buffer)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, experience_replay, optimizer):\n",
    "        \n",
    "        # Initialize atributes\n",
    "        self._optimizer = optimizer\n",
    "        \n",
    "        self.experience_replay = experience_replay\n",
    "        \n",
    "        # Initialize discount and exploration rate\n",
    "        self.epsilon = MAX_EPSILON\n",
    "        \n",
    "        # Build networks\n",
    "        self.primary_network = self._build_network()\n",
    "        self.primary_network.compile(loss='mse', optimizer=self._optimizer)\n",
    "\n",
    "        self.target_network = self._build_network()   \n",
    "   \n",
    "    def _build_network(self):\n",
    "        network = Sequential()\n",
    "        network.add(Dense(30, activation='relu', kernel_initializer=he_normal()))\n",
    "        network.add(Dense(30, activation='relu', kernel_initializer=he_normal()))\n",
    "        network.add(Dense(self._action_size))\n",
    "        \n",
    "        return network\n",
    "    \n",
    "    def align_epsilon(self, step):\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * step)\n",
    "    \n",
    "    def align_target_network(self):\n",
    "        for t, e in zip(self.target_network.trainable_variables, \n",
    "                    self.primary_network.trainable_variables): t.assign(t * (1 - TAU) + e * TAU)\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, self._action_size - 1)\n",
    "        else:\n",
    "            q_values = self.primary_network(state.reshape(1, -1))\n",
    "            return np.argmax(q_values)\n",
    "    \n",
    "    def store(self, state, action, reward, next_state, terminated):\n",
    "        self.experience_replay.store(state, action, reward, next_state, terminated)\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        if self.experience_replay.buffer_size < BATCH_SIZE * 3:\n",
    "            return 0\n",
    "        \n",
    "        batch = self.experience_replay.get_batch(batch_size)\n",
    "        states, actions, rewards, next_states = self.experience_replay.get_arrays_from_batch(batch)\n",
    "        \n",
    "        # Predict Q(s,a) and Q(s',a') given the batch of states\n",
    "        q_values_state = self.primary_network(states).numpy()\n",
    "        q_values_next_state = self.primary_network(next_states).numpy()\n",
    "        \n",
    "        # Copy the q_values_state into the target\n",
    "        target = q_values_state\n",
    "        updates = np.zeros(rewards.shape)\n",
    "                \n",
    "        valid_indexes = np.array(next_states).sum(axis=1) != 0\n",
    "        batch_indexes = np.arange(BATCH_SIZE)\n",
    "\n",
    "        action = np.argmax(q_values_next_state, axis=1)\n",
    "        q_next_state_target = self.target_network(next_states)\n",
    "        updates[valid_indexes] = rewards[valid_indexes] + \\\n",
    "            GAMMA * \\\n",
    "            q_next_state_target.numpy()[batch_indexes[valid_indexes], action[valid_indexes]]\n",
    "        \n",
    "        target[batch_indexes, actions] = updates\n",
    "        loss = self.primary_network.train_on_batch(states, target)\n",
    "\n",
    "        # update target network parameters slowly from primary network\n",
    "        self.align_target_network()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "class AgentTrainer():\n",
    "    def __init__(self, agent, enviroment):\n",
    "        self.agent = agent\n",
    "        self.enviroment = enviroment\n",
    "        \n",
    "    def _take_action(self, action):\n",
    "        next_state, reward, terminated, _ = self.enviroment.step(action) \n",
    "        next_state = next_state if not terminated else None\n",
    "        reward = np.random.normal(1.0, REWARD_STD)\n",
    "        return next_state, reward, terminated\n",
    "    \n",
    "    def _print_epoch_values(self, episode, total_epoch_reward, average_loss):\n",
    "        print(\"**********************************\")\n",
    "        print(f\"Episode: {episode} - Reward: {total_epoch_reward} - Average Loss: {average_loss:.3f}\")\n",
    "    \n",
    "    def train(self, num_of_episodes = 1000):\n",
    "        total_timesteps = 0  \n",
    "        \n",
    "        for episode in range(0, num_of_episodes):\n",
    "\n",
    "            # Reset the enviroment\n",
    "            state = self.enviroment.reset()\n",
    "\n",
    "            # Initialize variables\n",
    "            average_loss_per_episode = []\n",
    "            average_loss = 0\n",
    "            total_epoch_reward = 0\n",
    "\n",
    "            terminated = False\n",
    "\n",
    "            while not terminated:\n",
    "\n",
    "                # Run Action\n",
    "                action = agent.act(state)\n",
    "\n",
    "                # Take action    \n",
    "                next_state, reward, terminated = self._take_action(action)\n",
    "                agent.store(state, action, reward, next_state, terminated)\n",
    "                \n",
    "                loss = agent.train(BATCH_SIZE)\n",
    "                average_loss += loss\n",
    "\n",
    "                state = next_state\n",
    "                agent.align_epsilon(total_timesteps)\n",
    "                total_timesteps += 1\n",
    "\n",
    "                if terminated:\n",
    "                    average_loss /= total_epoch_reward\n",
    "                    average_loss_per_episode.append(average_loss)\n",
    "                    self._print_epoch_values(episode, total_epoch_reward, average_loss)\n",
    "                \n",
    "                # Real Reward is always 1 for Cart-Pole enviroment\n",
    "                total_epoch_reward +=1\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qnetwork.qpod_env import QPodEnvironment\n",
    "environment = QPodEnvironment(PodWorld())\n",
    "\n",
    "optimizer = Adam()\n",
    "experience_replay = ExperienceReplay(50000)\n",
    "agent = DDQNAgent(experience_replay, optimizer)\n",
    "agent_trainer = AgentTrainer(agent, enviroment)\n",
    "agent_trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
